{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Depression Detection: Full Model Pipeline\n",
        "\n",
        "This notebook demonstrates a complete pipeline for multimodal depression detection using deep learning. The workflow includes:\n",
        "\n",
        "- Cloning the project repository\n",
        "- Installing required dependencies\n",
        "- Mounting Google Drive and extracting the dataset\n",
        "- Preprocessing and fixing dataset paths\n",
        "- Creating custom dataset and model scripts\n",
        "- Training and evaluating the model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Clone Project Repository & Set Working Directory\n",
        "This cell clones the Multimodal Depression project from GitHub and sets the working directory to the encoder folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flyVXducjIAi",
        "outputId": "43de620d-d607-40ef-acd8-a74cdb9b5754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Multimodal-Depression'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 20 (delta 2), reused 14 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (20/20), 24.78 KiB | 3.54 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "/content/Multimodal-Depression/encoder\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Amcky/Multimodal-Depression.git\n",
        "%cd /content/Multimodal-Depression/encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### ðŸ› ï¸ Install Required Python Packages\n",
        "This cell installs all necessary Python libraries for model training and data processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0T6-A4XjMs6",
        "outputId": "bc38a0ac-268d-4aee-e0ba-74471686df42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting pytorch_lightning\n",
            "  Using cached pytorch_lightning-2.5.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting lightning\n",
            "  Using cached lightning-2.5.3-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (25.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.12.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.3-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m828.2/828.2 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.3-py3-none-any.whl (824 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m824.2/824.2 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning, lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-2.5.3 lightning-utilities-0.15.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 pytorch_lightning-2.5.3 torchmetrics-1.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision pandas numpy tqdm pytorch_lightning lightning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Mount Google Drive\n",
        "This cell mounts your Google Drive to access datasets and pre-trained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLS8qnfhwRfF",
        "outputId": "7833f798-41d3-4c00-e7b6-568e1bdeacdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### ðŸ”§ Additional Path Correction for CSV Files\n",
        "This cell further corrects any missing or misaligned CSV file paths in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a6s5AufhhyE",
        "outputId": "94da10cd-c963-4e1b-f290-0df99a09a74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unzipped dataset to /content\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/dataset.zip'\n",
        "unzip_dir = '/content'\n",
        "os.makedirs(unzip_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_dir)\n",
        "print(\"Unzipped dataset to\", unzip_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Unzip Dataset\n",
        "This cell extracts the dataset from Google Drive into the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-prjYTXkizrk",
        "outputId": "f5677240-0044-4a16-d090-36e354452c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset  drive\tMultimodal-Depression  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### List Dataset Contents\n",
        "This cell lists the files and folders in the dataset directory to verify extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS51AnXuzNAG",
        "outputId": "caaf3f44-85b0-45f5-a36a-401a71d9c85e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                         frames_path  \\\n",
            "0  /content/dataset/avec14/frames_align/train_Nor...   \n",
            "1  /content/dataset/avec14/frames_align/train_Fre...   \n",
            "2  /content/dataset/avec14/frames_align/train_Nor...   \n",
            "3  /content/dataset/avec14/frames_align/train_Fre...   \n",
            "4  /content/dataset/avec14/frames_align/train_Nor...   \n",
            "\n",
            "                                           faps_path  \\\n",
            "0  /content/dataset/avec14/faps/train_Northwind/2...   \n",
            "1  /content/dataset/avec14/faps/train_Freeform/20...   \n",
            "2  /content/dataset/avec14/faps/train_Northwind/2...   \n",
            "3  /content/dataset/avec14/faps/train_Freeform/20...   \n",
            "4  /content/dataset/avec14/faps/train_Northwind/2...   \n",
            "\n",
            "                                           rppg_path  label  \n",
            "0  /content/dataset/avec14/rppg_physformer/train_...      3  \n",
            "1  /content/dataset/avec14/rppg_physformer/train_...      3  \n",
            "2  /content/dataset/avec14/rppg_physformer/train_...      3  \n",
            "3  /content/dataset/avec14/rppg_physformer/train_...      3  \n",
            "4  /content/dataset/avec14/rppg_physformer/train_...     10  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "csv_path = \"/content/dataset/avec14/multimodal_labels.csv\"\n",
        "base_dir = \"/content\"\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "for col in [\"frames_path\", \"faps_path\", \"rppg_path\"]:\n",
        "    df[col] = df[col].apply(lambda p: os.path.abspath(os.path.join(base_dir, p)) if not os.path.isabs(p) else p)\n",
        "\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Fix Dataset Paths in CSV\n",
        "This cell updates the dataset CSV to ensure all file paths are absolute and correct for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tFTfZpR4YfM",
        "outputId": "5e326d65-7235-4906-b13f-96efb80eb7e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed CSV saved to /content/dataset/avec14/multimodal_labels_fixed.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "label_csv = \"/content/dataset/avec14/multimodal_labels.csv\"\n",
        "output_csv = \"/content/dataset/avec14/multimodal_labels_fixed.csv\"\n",
        "\n",
        "df = pd.read_csv(label_csv)\n",
        "\n",
        "def fix_path(path):\n",
        "    if not os.path.exists(path) and path.endswith(\".csv\"):\n",
        "        base, ext = os.path.splitext(path)\n",
        "        new_path = base + \"_video_aligned\" + ext\n",
        "        if os.path.exists(new_path):\n",
        "            return new_path\n",
        "    return path\n",
        "\n",
        "for col in df.columns:\n",
        "    if \"faps\" in col or \"rppg\" in col:\n",
        "        df[col] = df[col].apply(fix_path)\n",
        "\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"Fixed CSV saved to {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Create Custom Dataset Script\n",
        "This cell writes the custom dataset class (`dataset.py`) used for loading and preprocessing multimodal data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-sLjYVSqkvv"
      },
      "outputs": [],
      "source": [
        "new_code = \"\"\"\n",
        "# /content/Multimodal-Depression/encoder/dataset.py\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import pytorch_lightning as pl\n",
        "import warnings\n",
        "\n",
        "# ---------- Utility functions ----------\n",
        "def load_image(path):\n",
        "    return Image.open(path).convert(\"RGB\")\n",
        "\n",
        "def load_csv_features(path):\n",
        "    df = pd.read_csv(path)\n",
        "    arr = df.values.astype('float32')\n",
        "\n",
        "    # Replace NaN/Inf with 0\n",
        "    if not np.isfinite(arr).all():\n",
        "        arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    # arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # Standardize per sequence (T x D)\n",
        "    if arr.shape[0] > 1:\n",
        "        mean = arr.mean(axis=0, keepdims=True)\n",
        "        std = arr.std(axis=0, keepdims=True) + 1e-6\n",
        "        arr = (arr - mean) / std\n",
        "\n",
        "    # Clip extreme values to prevent blow-ups\n",
        "    clip_threshold = 10.0\n",
        "    max_val = np.max(arr)\n",
        "    min_val = np.min(arr)\n",
        "    if max_val > clip_threshold or min_val < -clip_threshold:\n",
        "        arr = np.clip(arr, -clip_threshold, clip_threshold)\n",
        "\n",
        "    return torch.tensor(arr, dtype=torch.float32)\n",
        "\n",
        "# Map between args.modalities and CSV column names\n",
        "MODALITY_TO_COLUMN = {\n",
        "    'frames_align': 'frames_path',\n",
        "    'faps': 'faps_path',\n",
        "    'rppg_physformer': 'rppg_path',\n",
        "}\n",
        "\n",
        "MODALITY_TYPES = {\n",
        "    'frames_align': 'image',\n",
        "    'faps': 'csv',\n",
        "    'rppg_physformer': 'csv',\n",
        "}\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, args, labeldata, transform=None, stage='train'):\n",
        "        self.args = args\n",
        "        self.labeldata = labeldata.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.modalities = args.modalities  # preserve order requested by the model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labeldata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.labeldata.iloc[idx]\n",
        "        sample = {}\n",
        "\n",
        "        for m in self.modalities:\n",
        "            col = MODALITY_TO_COLUMN[m]\n",
        "            path = row[col]\n",
        "\n",
        "            if MODALITY_TYPES[m] == 'image':\n",
        "                file_list = sorted(glob.glob(os.path.join(path, '*.jpg')))\n",
        "                if not file_list:\n",
        "                    raise FileNotFoundError(f\"No .jpg files in {path}\")\n",
        "                img = load_image(file_list[0])  # first frame\n",
        "                img = self.transform(img) if self.transform else T.ToTensor()(img)\n",
        "                sample[m] = img  # [3,H,W]\n",
        "\n",
        "            else:  # CSV modalities\n",
        "                if not os.path.isfile(path):\n",
        "                    raise FileNotFoundError(f\"CSV file not found: {path}\")\n",
        "                csv_tensor = load_csv_features(path)  # [T,D]\n",
        "                sample[m] = csv_tensor\n",
        "\n",
        "        score = torch.tensor(float(row['label']), dtype=torch.float32)\n",
        "        vid_id = os.path.basename(str(row[MODALITY_TO_COLUMN['frames_align']]))\n",
        "        return sample, score, vid_id\n",
        "\n",
        "# ---------- DataModule ----------\n",
        "class VideoRegressionDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.batch_size = args.batch_size\n",
        "        self.num_workers = args.num_workers\n",
        "        self.labeldata = pd.read_csv(args.label_file)\n",
        "\n",
        "        self.base_transform = T.Compose([\n",
        "            T.Resize((112, 112)),  # resize first for consistency\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        self.train_dataset = VideoDataset(args, self.labeldata, transform=self.base_transform, stage='train')\n",
        "        self.val_dataset = self.train_dataset\n",
        "        self.test_dataset = self.val_dataset\n",
        "\n",
        "    def _loader(self, ds, shuffle):\n",
        "        return DataLoader(\n",
        "            ds, batch_size=self.batch_size, shuffle=shuffle,\n",
        "            num_workers=min(self.num_workers, 2),  # heed Colab worker warning\n",
        "            pin_memory=True, collate_fn=self._collate\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _collate(batch):\n",
        "        # batch: list of (sample_dict, score, vid_id)\n",
        "        samples, scores, ids = zip(*batch)\n",
        "        batch_dict = {}\n",
        "        for k in samples[0].keys():\n",
        "            v = samples[0][k]\n",
        "            if isinstance(v, torch.Tensor) and v.ndim == 3:\n",
        "                batch_dict[k] = torch.stack([s[k] for s in samples], dim=0)\n",
        "            else:  # csv [T,D]\n",
        "                ts = [s[k] for s in samples]\n",
        "                T_max = max(t.shape[0] for t in ts)\n",
        "                Ds = ts[0].shape[1]\n",
        "                out = torch.zeros(len(ts), T_max, Ds, dtype=ts[0].dtype)\n",
        "                for i, t in enumerate(ts):\n",
        "                    out[i, :t.shape[0]] = t\n",
        "                batch_dict[k] = out\n",
        "        scores = torch.stack(scores, dim=0)  # [B]\n",
        "        return batch_dict, scores, list(ids)\n",
        "\n",
        "    def train_dataloader(self): return self._loader(self.train_dataset, True)\n",
        "    def val_dataloader(self):   return self._loader(self.val_dataset, False)\n",
        "    def test_dataloader(self):  return self._loader(self.test_dataset, False)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/Multimodal-Depression/encoder/dataset.py\", \"w\") as f:\n",
        "    f.write(new_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Create Model Script\n",
        "This cell writes the model architecture (`model.py`) for multimodal regression using deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDlvZY1rrbXY"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_code = \"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pytorch_lightning import LightningModule\n",
        "from torchmetrics import MeanAbsoluteError, MeanSquaredError\n",
        "from backbones import iresnet\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "# -----------------------------\n",
        "# Extra Metrics (PCC & CCC)\n",
        "# -----------------------------\n",
        "def pearson_corr(x, y, eps=1e-8):\n",
        "    x = x.view(-1)\n",
        "    y = y.view(-1)\n",
        "    vx = x - x.mean()\n",
        "    vy = y - y.mean()\n",
        "    num = torch.sum(vx * vy)\n",
        "    den = torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) + eps\n",
        "    return num / den\n",
        "\n",
        "def concordance_corr(x, y, eps=1e-8):\n",
        "    x = x.view(-1)\n",
        "    y = y.view(-1)\n",
        "    mx, my = x.mean(), y.mean()\n",
        "    vx, vy = x.var(unbiased=True), y.var(unbiased=True)\n",
        "    cov = ((x - mx) * (y - my)).mean()\n",
        "    return (2 * cov) / (vx + vy + (mx - my) ** 2 + eps)\n",
        "\n",
        "# -----------------------------\n",
        "# Image Feature Extractor\n",
        "# -----------------------------\n",
        "class ImageFeatureExtractor(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.model = iresnet.iresnet50(pretrained=False)\n",
        "\n",
        "        if not getattr(args, 'ablation', None) == 'no_pretrain':\n",
        "            ckpt = getattr(args, \"webface_ckpt\", \"/content/drive/MyDrive/webface_r50.pth\")\n",
        "            if getattr(args, 'pretrain', None) == 'webface' and os.path.isfile(ckpt):\n",
        "                state = torch.load(ckpt, map_location='cpu')\n",
        "                self.model.load_state_dict(state, strict=False)\n",
        "\n",
        "        for m in self.model.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                m.eval()\n",
        "\n",
        "        self.out_dim = self.model.fc.out_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# -----------------------------\n",
        "# CSV Feature Extractor\n",
        "# -----------------------------\n",
        "class CSVFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=256, out_dim=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, out_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 3:\n",
        "            x = x.mean(dim=1)  # temporal mean pooling\n",
        "        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        return self.net(x)\n",
        "\n",
        "# -----------------------------\n",
        "# Cross-Modal Transformer Fusion\n",
        "# -----------------------------\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=6, num_layers=2, mlp_ratio=4.0,\n",
        "                 dropout=0.1, attn_dropout=0.1, use_norm=True):\n",
        "        super().__init__()\n",
        "        self.use_norm = use_norm\n",
        "        self.ln = nn.LayerNorm(dim) if use_norm else nn.Identity()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=int(mlp_ratio * dim),\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln(x)\n",
        "        x = self.encoder(x)\n",
        "        return x\n",
        "\n",
        "# -----------------------------\n",
        "# Main Model\n",
        "# -----------------------------\n",
        "class VideoRegressionModel(LightningModule):\n",
        "    def __init__(self, args, csv_input_dims=None):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=['csv_input_dims'])\n",
        "        self.args = args\n",
        "        self.modalities = args.modalities\n",
        "\n",
        "        attn_heads  = getattr(args, \"attn_heads\", 6)\n",
        "        attn_layers = getattr(args, \"attn_layers\", 2)\n",
        "        fusion_dim  = getattr(args, \"fusion_dim\", 512)\n",
        "\n",
        "        if args.ablation == 'smaller_fusion_dim':\n",
        "            fusion_dim = 128\n",
        "\n",
        "        # -------------------------------\n",
        "        # ABLATION: modality removal\n",
        "        # -------------------------------\n",
        "        if args.ablation == 'frames_only':\n",
        "            self.modalities = ['frames_align']\n",
        "        elif args.ablation == 'csv_only':\n",
        "            self.modalities = [m for m in args.modalities if m in ('faps', 'rppg_physformer')]\n",
        "\n",
        "        self.extractors = nn.ModuleDict()\n",
        "        raw_dims = []\n",
        "        for m in self.modalities:\n",
        "            if m == 'frames_align':\n",
        "                self.extractors[m] = ImageFeatureExtractor(args)\n",
        "                raw_dims.append(self.extractors[m].out_dim)\n",
        "            elif m in ('faps', 'rppg_physformer'):\n",
        "                if csv_input_dims and m in csv_input_dims:\n",
        "                    in_dim = csv_input_dims[m]\n",
        "                else:\n",
        "                    in_dim = 29 if m == 'faps' else 64\n",
        "                self.extractors[m] = CSVFeatureExtractor(\n",
        "                    input_dim=in_dim, hidden=256, out_dim=128, dropout=0.1\n",
        "                )\n",
        "                raw_dims.append(self.extractors[m].out_dim)\n",
        "\n",
        "        # -------------------------------\n",
        "        # ABLATION: projectors\n",
        "        # -------------------------------\n",
        "        self.projectors = nn.ModuleDict()\n",
        "        if args.ablation == 'no_projectors':\n",
        "            for m, d in zip(self.modalities, raw_dims):\n",
        "                self.projectors[m] = nn.Identity()\n",
        "        elif args.ablation == 'shared_projector':\n",
        "            first_dim = raw_dims[0]\n",
        "            shared_proj = nn.Sequential(nn.Linear(first_dim, fusion_dim), nn.GELU())\n",
        "            for m in self.modalities:\n",
        "                self.projectors[m] = shared_proj\n",
        "        else:\n",
        "            for m, d in zip(self.modalities, raw_dims):\n",
        "                if d != fusion_dim:\n",
        "                    proj = nn.Linear(d, fusion_dim)\n",
        "                    nn.init.xavier_uniform_(proj.weight)\n",
        "                    if proj.bias is not None:\n",
        "                        nn.init.zeros_(proj.bias)\n",
        "                    self.projectors[m] = nn.Sequential(proj, nn.GELU())\n",
        "                else:\n",
        "                    self.projectors[m] = nn.Identity()\n",
        "\n",
        "        # -------------------------------\n",
        "        # ABLATION: attention fusion\n",
        "        # -------------------------------\n",
        "        if args.ablation == 'no_attention':\n",
        "            self.attention_fusion = nn.Identity()\n",
        "        elif args.ablation == 'uni_modal_attention':\n",
        "            self.attention_fusion = nn.ModuleList([\n",
        "                CrossModalAttention(\n",
        "                    dim=fusion_dim,\n",
        "                    num_heads=attn_heads,\n",
        "                    num_layers=attn_layers,\n",
        "                    dropout=getattr(args, \"attn_dropout\", 0.1),\n",
        "                    attn_dropout=getattr(args, \"attn_dropout\", 0.1),\n",
        "                    use_norm=not (args.ablation == 'no_norm')\n",
        "                ) for _ in self.modalities\n",
        "            ])\n",
        "        else:\n",
        "            if args.ablation == 'attn_1layer':\n",
        "                attn_layers = 1\n",
        "            elif args.ablation == 'attn_2heads':\n",
        "                attn_heads = 2\n",
        "            use_norm = not (args.ablation == 'no_norm')\n",
        "            self.attention_fusion = CrossModalAttention(\n",
        "                dim=fusion_dim,\n",
        "                num_heads=attn_heads,\n",
        "                num_layers=attn_layers,\n",
        "                mlp_ratio=4.0,\n",
        "                dropout=getattr(args, \"attn_dropout\", 0.1),\n",
        "                attn_dropout=getattr(args, \"attn_dropout\", 0.1),\n",
        "                use_norm=use_norm\n",
        "            )\n",
        "\n",
        "        # -------------------------------\n",
        "        # Regression head\n",
        "        # -------------------------------\n",
        "        if args.ablation == 'simple_regressor':\n",
        "            self.regressor = nn.Linear(fusion_dim, 1)\n",
        "        else:\n",
        "            dropout_rate = 0.0 if args.ablation == 'no_dropout' else args.dropout_rate\n",
        "            self.regressor = nn.Sequential(\n",
        "                nn.LayerNorm(fusion_dim),\n",
        "                nn.Linear(fusion_dim, 128),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout_rate),\n",
        "                nn.Linear(128, 1),\n",
        "            )\n",
        "\n",
        "        # -------------------------------\n",
        "        # ABLATION: frozen extractors\n",
        "        # -------------------------------\n",
        "        if args.ablation == 'frozen_extractors':\n",
        "            for p in self.extractors.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        self.mae_metric = MeanAbsoluteError()\n",
        "        self.mse_metric = MeanSquaredError()\n",
        "        self.test_preds, self.test_tgts = [], []\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, batch_dict):\n",
        "        tokens = []\n",
        "        for m in self.modalities:\n",
        "            feat = self.extractors[m](batch_dict[m])\n",
        "            feat = torch.nan_to_num(feat, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            feat = self.projectors[m](feat)\n",
        "            tokens.append(feat)\n",
        "\n",
        "        if self.args.ablation == 'uni_modal_attention':\n",
        "            attn_outs = []\n",
        "            for feat, attn in zip(tokens, self.attention_fusion):\n",
        "                feat = feat.unsqueeze(1)\n",
        "                attn_outs.append(attn(feat).squeeze(1))\n",
        "            x = torch.stack(attn_outs, dim=1)\n",
        "        else:\n",
        "            x = torch.stack(tokens, dim=1)\n",
        "            x = self.attention_fusion(x)\n",
        "\n",
        "        fused = x.mean(dim=1)\n",
        "        yhat = self.regressor(fused)\n",
        "        return yhat\n",
        "\n",
        "    def _step(self, batch, stage):\n",
        "        x_dict, y, _ = batch\n",
        "        y = y.view(-1, 1)\n",
        "        y_hat = self.forward(x_dict)\n",
        "        loss = F.mse_loss(y_hat, y)\n",
        "\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            self.log(f\"{stage}_loss\", 0.0, prog_bar=True)\n",
        "            return torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "        self.log(f'{stage}_loss', loss, prog_bar=True, on_epoch=True, batch_size=y.size(0))\n",
        "        self.log(f'{stage}_mae',  self.mae_metric(y_hat, y), prog_bar=True, on_epoch=True, batch_size=y.size(0))\n",
        "        self.log(f'{stage}_rmse', torch.sqrt(self.mse_metric(y_hat, y)), prog_bar=False, on_epoch=True, batch_size=y.size(0))\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._step(batch, 'train')\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        return self._step(batch, 'val')\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch, 'test')\n",
        "        x_dict, y, _ = batch\n",
        "        y_hat = self.forward(x_dict).detach()\n",
        "        self.test_preds.append(y_hat.cpu())\n",
        "        self.test_tgts.append(y.view(-1,1).cpu())\n",
        "        return loss\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        if self.test_preds:\n",
        "            yhat = torch.cat(self.test_preds, dim=0)\n",
        "            yt   = torch.cat(self.test_tgts, dim=0)\n",
        "            pcc = pearson_corr(yhat, yt)\n",
        "            ccc = concordance_corr(yhat, yt)\n",
        "            self.log('test_pcc', pcc, prog_bar=True)\n",
        "            self.log('test_ccc', ccc, prog_bar=True)\n",
        "\n",
        "            total_params = sum(p.numel() for p in self.parameters())\n",
        "            trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "            self.print(f\"#Params total: {total_params/1e6:.2f}M, trainable: {trainable_params/1e6:.2f}M\")\n",
        "\n",
        "        self.test_preds.clear()\n",
        "        self.test_tgts.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.parameters(),\n",
        "            lr=self.args.learning_rate,\n",
        "            weight_decay=self.args.weight_decay\n",
        "        )\n",
        "\n",
        "        def lr_lambda(epoch):\n",
        "            warmup = getattr(self.args, \"warmup_epochs\", 5)\n",
        "            if epoch < warmup:\n",
        "                return float(epoch + 1) / float(max(1, warmup))\n",
        "            progress = (epoch - warmup) / float(max(1, self.args.max_epochs - warmup))\n",
        "            return 0.5 * (1.0 + math.cos(progress * 3.1415926535))\n",
        "\n",
        "        scheduler = {\n",
        "            'scheduler': LambdaLR(optimizer, lr_lambda=lr_lambda),\n",
        "            'interval': 'epoch',\n",
        "            'frequency': 1,\n",
        "        }\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/Multimodal-Depression/encoder/model.py\", \"w\") as f:\n",
        "    f.write(new_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Create Main Training Script\n",
        "This cell writes the main script (`main.py`) to train, validate, and test the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dQFnwTLrj9z"
      },
      "outputs": [],
      "source": [
        "\n",
        "new_code = \"\"\"\n",
        "# /content/Multimodal-Depression/encoder/main.py\n",
        "import glob, os, argparse, re\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
        "from lightning.pytorch.loggers import CSVLogger\n",
        "from dataset import VideoRegressionDataModule\n",
        "from model import VideoRegressionModel\n",
        "\n",
        "seed_everything(1)\n",
        "\n",
        "def find_min_mae_file(file_list):\n",
        "    min_mae, min_file = float('inf'), None\n",
        "    mae_pattern = re.compile(r'val_mae=([\\\\d.]+)')\n",
        "    for fp in file_list:\n",
        "        m = mae_pattern.search(fp)\n",
        "        if m:\n",
        "            v = float(m.group(1))\n",
        "            if v < min_mae:\n",
        "                min_mae, min_file = v, fp\n",
        "    return min_file\n",
        "\n",
        "def find_latest_checkpoint(latest_dir: str):\n",
        "    last_ckpt = os.path.join(latest_dir, \"last.ckpt\")\n",
        "    if os.path.isfile(last_ckpt):\n",
        "        return last_ckpt\n",
        "    ckpts = glob.glob(os.path.join(latest_dir, \"*.ckpt\"))\n",
        "    if not ckpts:\n",
        "        return None\n",
        "    def epoch_num(p):\n",
        "        m = re.search(r\"epoch(\\\\d+)\", os.path.basename(p))\n",
        "        return int(m.group(1)) if m else -1\n",
        "    ckpts.sort(key=epoch_num)\n",
        "    return ckpts[-1] if ckpts else None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data_dir', type=str, default='/content/dataset/avec14')\n",
        "    parser.add_argument('--label_file', type=str, default='/content/dataset/avec14/multimodal_labels_fixed.csv')\n",
        "    parser.add_argument('--train_data', nargs='+', default=['AVEC2014-train'])\n",
        "    parser.add_argument('--val_data', nargs='+', default=['AVEC2014-dev'])\n",
        "    parser.add_argument('--test_data', nargs='+', default=['AVEC2014-test'])\n",
        "    parser.add_argument('--modalities', nargs='+', default=['frames_align', 'faps', 'rppg_physformer'])\n",
        "    parser.add_argument('--num_frames', type=int, default=1)\n",
        "    parser.add_argument('--frame_interval', type=int, default=1)\n",
        "    parser.add_argument('--pretrain', type=str, default='webface')\n",
        "\n",
        "    parser.add_argument('--save_dir', type=str,\n",
        "                        default='/content/drive/MyDrive/Multimodal-Depression')\n",
        "\n",
        "    parser.add_argument('--remove_rate', type=float, default=0.1)\n",
        "    parser.add_argument('--batch_size', type=int, default=128)\n",
        "    parser.add_argument('--num_workers', type=int, default=4)\n",
        "    parser.add_argument('--max_epochs', type=int, default=300)\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4)\n",
        "    parser.add_argument('--weight_decay', type=float, default=1e-2)\n",
        "    parser.add_argument('--dropout_rate', type=float, default=0.7)\n",
        "    parser.add_argument('--fusion_dim', type=int, default=512)\n",
        "    parser.add_argument('--attn_heads', type=int, default=6)\n",
        "    parser.add_argument('--attn_layers', type=int, default=2)\n",
        "    parser.add_argument('--attn_dropout', type=float, default=0.1)\n",
        "    parser.add_argument('--warmup_epochs', type=int, default=5)\n",
        "    parser.add_argument('--no_resume', action='store_true', help='Disable auto-resume from latest/last.ckpt')\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--ablation',\n",
        "        type=str,\n",
        "        default='none',\n",
        "        choices=[\n",
        "            'none', 'no_attention', 'attn_1layer',\n",
        "            'no_projectors', 'shared_projector',\n",
        "            'frozen_extractors',\n",
        "            'simple_regressor',\n",
        "            'no_pretrain', 'smaller_fusion_dim',\n",
        "            'no_norm', 'uni_modal_attention'\n",
        "        ],\n",
        "        help='Ablation study mode'\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "    args.save_dir = os.path.abspath(args.save_dir)\n",
        "    best_dir   = os.path.join(args.save_dir, \"best\")\n",
        "    latest_dir = os.path.join(args.save_dir, \"latest\")\n",
        "    logs_dir   = os.path.join(args.save_dir, \"logs\")\n",
        "    for d in [best_dir, latest_dir, logs_dir]:\n",
        "       os.makedirs(d, exist_ok=True)\n",
        "       print(f\"Folder ready: {d}\")\n",
        "\n",
        "    datamodule = VideoRegressionDataModule(args)\n",
        "    model = VideoRegressionModel(args, csv_input_dims={'faps': 29, 'rppg_physformer': 23})\n",
        "\n",
        "    best_checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=best_dir,\n",
        "        monitor=\"val_mae\",\n",
        "        mode='min',\n",
        "        save_top_k=1,\n",
        "        filename='best-{epoch:03d}-{val_mae:.2f}-{val_rmse:.2f}'\n",
        "    )\n",
        "    # Save EVERY epoch + keep rolling \"last.ckpt\"\n",
        "    latest_checkpoint_callback = ModelCheckpoint(\n",
        "        dirpath=latest_dir,\n",
        "        save_top_k=-1,\n",
        "        every_n_epochs=50,\n",
        "        save_last=True,\n",
        "        filename='epoch{epoch:03d}'\n",
        "    )\n",
        "\n",
        "    print(f\"Lightning will save BEST checkpoints to: {best_checkpoint_callback.dirpath}\")\n",
        "    print(f\"Lightning will save LATEST checkpoints to: {latest_checkpoint_callback.dirpath}\")\n",
        "    print(f\"Logger files will go to: {logs_dir}\")\n",
        "\n",
        "    early_stop_callback = EarlyStopping(monitor='val_loss', mode='min', patience=40)\n",
        "    lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=args.max_epochs,\n",
        "        accelerator=\"auto\",\n",
        "        logger=CSVLogger(save_dir=logs_dir),\n",
        "        callbacks=[best_checkpoint_callback, latest_checkpoint_callback, early_stop_callback, lr_monitor],\n",
        "        gradient_clip_val=1.0,\n",
        "        gradient_clip_algorithm=\"norm\"\n",
        "    )\n",
        "\n",
        "    ckpt_path = None\n",
        "    if not args.no_resume:\n",
        "        ckpt_path = find_latest_checkpoint(latest_dir)\n",
        "        if ckpt_path and os.path.isfile(ckpt_path):\n",
        "            print(f\">>> Resuming from checkpoint: {ckpt_path}\")\n",
        "        else:\n",
        "            ckpt_path = None\n",
        "\n",
        "    trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)\n",
        "\n",
        "    test_ckpt = best_checkpoint_callback.best_model_path\n",
        "    if not test_ckpt or not os.path.isfile(test_ckpt):\n",
        "        test_ckpt = find_latest_checkpoint(latest_dir)\n",
        "\n",
        "    if test_ckpt and os.path.isfile(test_ckpt):\n",
        "        print(f\">>> Testing with checkpoint: {test_ckpt}\")\n",
        "        best_model = VideoRegressionModel.load_from_checkpoint(test_ckpt, csv_input_dims={'faps': 29, 'rppg_physformer': 23}, strict=False)\n",
        "        trainer.test(best_model, datamodule=datamodule)\n",
        "    else:\n",
        "        print(\">>> No checkpoint found, testing current in-memory model.\")\n",
        "        trainer.test(model, datamodule=datamodule)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/Multimodal-Depression/encoder/main.py\", \"w\") as f:\n",
        "    f.write(new_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python main.py \\\n",
        "  --data_dir /content/dataset/avec14 \\\n",
        "  --label_file /content/dataset/avec14/multimodal_labels_fixed.csv \\\n",
        "  --train_data AVEC2014-train \\\n",
        "  --val_data AVEC2014-dev \\\n",
        "  --test_data AVEC2014-test \\\n",
        "  --modalities frames_align faps \\\n",
        "  --num_frames 1 \\\n",
        "  --frame_interval 1 \\\n",
        "  --pretrain webface \\\n",
        "  --save_dir /content/drive/MyDrive/Multimodal-Depression \\\n",
        "  --remove_rate 0.1 \\\n",
        "  --batch_size 2 \\\n",
        "  --num_workers 4 \\\n",
        "  --max_epochs 300 \\\n",
        "  --learning_rate 1e-5 \\\n",
        "  --weight_decay 1e-4 \\\n",
        "  --dropout_rate 0.7 \\\n",
        "  --fusion_dim 512 \\\n",
        "  --attn_heads 8 \\\n",
        "  --attn_layers 4 \\\n",
        "  --attn_dropout 0.1 \\\n",
        "  --ablation none\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
